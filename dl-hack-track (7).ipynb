{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline \n",
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D,MaxPool2D,DepthwiseConv2D,Flatten,Dense,Dropout,AveragePooling2D,BatchNormalization,GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "img_height=512\n",
    "img_width=512\n",
    "batch_size=64\n",
    "directory=\"../input//dl-hack-track-1-cv/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen=ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "    horizontal_flip=False,\n",
    "    data_format=\"channels_last\",\n",
    "    validation_split=0.2,\n",
    "    dtype=tf.float32\n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28000 images belonging to 2 classes.\n",
      "Found 7000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data=datagen.flow_from_directory(\n",
    "    \"../input//dl-hack-track-1-cv/train/\",\n",
    "    target_size=(512,512),\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='training'\n",
    "    \n",
    ")\n",
    "val_data=datagen.flow_from_directory(\n",
    "    \"../input//dl-hack-track-1-cv/train/\",\n",
    "    target_size=(512,512),\n",
    "    batch_size=batch_size,\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    subset='validation'\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_28 (Conv2D)           (None, 512, 512, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 256, 256, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 128, 128, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 32, 32, 256)       131328    \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 32, 32, 256)       262400    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               8388736   \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 8,887,712\n",
      "Trainable params: 8,886,752\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "Epoch 1/4\n",
      "438/438 [==============================] - 1020s 2s/step - loss: 0.9655 - accuracy: 0.7108 - val_loss: 1.1450 - val_accuracy: 0.5210\n",
      "Epoch 2/4\n",
      "438/438 [==============================] - 1074s 2s/step - loss: 0.2821 - accuracy: 0.8866 - val_loss: 0.1811 - val_accuracy: 0.9294\n",
      "Epoch 3/4\n",
      "438/438 [==============================] - 1118s 3s/step - loss: 0.2265 - accuracy: 0.9084 - val_loss: 0.1517 - val_accuracy: 0.9426\n",
      "Epoch 4/4\n",
      "438/438 [==============================] - 1145s 3s/step - loss: 0.1643 - accuracy: 0.9345 - val_loss: 0.1333 - val_accuracy: 0.9487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff031663190>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= keras.Sequential()\n",
    "# input shape is shape of images\n",
    "model.add(keras.Input(shape=(512,512,3)))\n",
    "model.add(Conv2D(32,(2,2),padding='same'))\n",
    "#now step down the image dimentions \n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "# from here the MTCNN architechure is take with slight modeification of channnels\n",
    "model.add(Conv2D(32,(3,3),activation = 'relu',padding='same')) \n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(256, (2, 2), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (2, 2), activation='relu', padding='same'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10,activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(0.000005),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "    )\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_data,epochs=4,validation_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "438/438 [==============================] - 1089s 2s/step - loss: 0.1256 - accuracy: 0.9513 - val_loss: 0.1033 - val_accuracy: 0.9587\n",
      "Epoch 3/4\n",
      "438/438 [==============================] - 1107s 3s/step - loss: 0.1088 - accuracy: 0.9589 - val_loss: 0.0953 - val_accuracy: 0.9630\n",
      "Epoch 4/4\n",
      "438/438 [==============================] - 1109s 3s/step - loss: 0.0966 - accuracy: 0.9648 - val_loss: 0.0916 - val_accuracy: 0.9659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff030841990>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data,epochs=4,validation_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflow(i):\n",
    "    x=MaxPool2D((2, 2))(i)\n",
    "    x=DepthwiseConv2D((3,3),padding=\"same\")(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    #x=Conv2D(3,(3,3),padding=\"same\",activation=\"relu\")(x)\n",
    "    return x\n",
    "def outflow(x):\n",
    "    x=Flatten()(x)\n",
    "    x=Dense(64,activation='relu')(x)\n",
    "    x=Dense(16,activation='relu')(x)\n",
    "    x=Dense(2,activation=\"sigmoid\")(x)\n",
    "    return x\n",
    "    \n",
    "def midflow(x):\n",
    "    x=Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\")(x)\n",
    "    x=Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\")(x)\n",
    "    x=MaxPool2D(pool_size=(2,2),strides=(2,2))(x)\n",
    "    \n",
    "    #========================================================================\n",
    "    y=Conv2D(filters=64,kernel_size=(1,1), padding=\"same\", activation=\"relu\")(x)\n",
    "    y=DepthwiseConv2D(kernel_size=(2,2),strides=(2,2), padding=\"same\", activation=\"relu\")(y)\n",
    "\n",
    "    #=========================================================================\n",
    "    x=Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x=DepthwiseConv2D(kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x=MaxPool2D(pool_size=(2,2),strides=(2,2))(x)\n",
    "    x=layers.Concatenate()([x,y])\n",
    "\n",
    "    x=Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x=Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x=Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x=MaxPool2D(pool_size=(2,2),strides=(2,2))(x)\n",
    "    \n",
    "    #============================================================================\\\n",
    "    \n",
    "    y=Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(y)\n",
    "    y=Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(y)\n",
    "    y=DepthwiseConv2D(kernel_size=(2,2),strides=(2,2), padding=\"same\", activation=\"relu\")(y)\n",
    "    y=MaxPool2D(pool_size=(2,2),strides=(2,2))(y)\n",
    "    \n",
    "    \n",
    "    z=Conv2D(filters=256,kernel_size=(1,1), padding=\"same\", activation=\"relu\")(x)\n",
    "    z=Conv2D(filters=256,kernel_size=(1,1), padding=\"same\", activation=\"relu\")(z)\n",
    "    z=DepthwiseConv2D(kernel_size=(2,2),strides=(2,2), padding=\"same\", activation=\"relu\")(z)\n",
    "    \n",
    "    #=============================================================================\n",
    "    x=Conv2D(filters=576, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x=Conv2D(filters=576, kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x=Conv2D(filters=576,kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x=MaxPool2D(pool_size=(2,2),strides=(2,2))(x)\n",
    "    \n",
    "    x=layers.Concatenate()([x,y,z])\n",
    "    x=Conv2D(filters=896, kernel_size=(3,3), activation=\"relu\")(x)\n",
    "    x=Conv2D(filters=896, kernel_size=(2,2), padding=\"same\", activation=\"relu\")(x)\n",
    "    x=DepthwiseConv2D(kernel_size=(3,3), padding=\"same\", activation=\"relu\")(x)\n",
    "    x=MaxPool2D(pool_size=(2,2),strides=(2,2))(x)\n",
    "    return x    \n",
    "\n",
    "inputs=keras.Input(shape=(512,512,3))\n",
    "y=inflow(inputs)\n",
    "y=midflow(y)\n",
    "outputs=outflow(y)\n",
    "Model= keras.Model(inputs,outputs)\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.compile(\n",
    "       optimizer=tf.keras.optimizers.Adam(0.000005),\n",
    "       loss='categorical_crossentropy',\n",
    "       metrics=['accuracy']\n",
    ")\n",
    "#Model.load_weights(\"transfer.h5\")\n",
    "Model.fit(train_data,epochs=3\n",
    "          ,validation_data=val_data\n",
    "         )\n",
    "Model.save(\"transfer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.compile(\n",
    "       optimizer=tf.keras.optimizers.Adam(0.000004),\n",
    "       loss='categorical_crossentropy',\n",
    "       metrics=['accuracy']\n",
    ")\n",
    "#Model.load_weights(\"transfer.h5\")\n",
    "Model.fit(train_data,epochs=4\n",
    "          ,validation_data=val_data\n",
    "         )\n",
    "Model.save(\"transfer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.compile(\n",
    "   optimizer=tf.keras.optimizers.Adam(0.0000015),\n",
    "   loss='categorical_crossentropy',\n",
    "   metrics=['accuracy']\n",
    "\n",
    ")\n",
    "Model.load_weights(\"transfer.h5\")\n",
    "Model.fit(train_data,epochs=1,validation_data=val_data)\n",
    "Model.save(\"transfer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify(Id):\n",
    "    x=predictwithId(Id)\n",
    "    if x>0.995:\n",
    "        return 0.99\n",
    "    if x<0.0025:\n",
    "        return 0.005\n",
    "    return float(x)\n",
    "    \n",
    "\n",
    "def predictwithId(Id):\n",
    "    path='../input/dl-hack-track-1-cv/test/'+str(Id)+\".png\"\n",
    "    image = tf.keras.preprocessing.image.load_img(path,\n",
    "                                              grayscale=False, \n",
    "                                              color_mode=\"rgb\", \n",
    "                                              target_size=None,\n",
    "                                              interpolation=\"nearest\")\n",
    "    input_arr = keras.preprocessing.image.img_to_array(image)\n",
    "    input_arr = np.array(input_arr)/255  # Convert single image to a batch.\n",
    "    result=model.predict(np.reshape(input_arr,(1,512,512,3)))[:,1]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009194078156724572"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='../input/dl-hack-track-1-cv/train/fake/'+str(4)+\".png\"\n",
    "image = tf.keras.preprocessing.image.load_img(path,\n",
    "                                              grayscale=False, \n",
    "                                              color_mode=\"rgb\", \n",
    "                                              target_size=None,\n",
    "                                              interpolation=\"nearest\")\n",
    "input_arr = keras.preprocessing.image.img_to_array(image)\n",
    "input_arr = np.array(input_arr)/255  # Convert single image to a batch.\n",
    "result=model.predict(np.reshape(input_arr,(1,512,512,3)))[:,1]\n",
    "float(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "submission=pd.read_csv(\"../input/dl-hack-track-1-cv/sample_submission.csv\")\n",
    "submission['p_real']=submission['id'].apply(classify)\n",
    "print(len(submission))\n",
    "submission.set_index('id').to_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Model.compile(\n",
    "       optimizer=tf.keras.optimizers.Adam(0.00005),\n",
    "       loss='categorical_crossentropy',\n",
    "       metrics=['accuracy']\n",
    ")\n",
    "#Model.load_weights(\"transfer.h5\")\n",
    "Model.fit(train_data,epochs=3\n",
    "          ,validation_data=val_data\n",
    "         )\n",
    "Model.save(\"transfer.h5\")\n",
    "\n",
    "Epoch 1/3\n",
    "219/219 [==============================] - 1196s 5s/step - loss: 0.5817 - accuracy: 0.6411 - val_loss: 0.6919 - val_accuracy: 0.5164\n",
    "Epoch 2/3\n",
    "219/219 [==============================] - 1223s 6s/step - loss: 0.2059 - accuracy: 0.9193 - val_loss: 0.2684 - val_accuracy: 0.8927\n",
    "Epoch 3/3\n",
    "219/219 [==============================] - 1177s 5s/step - loss: 0.1251 - accuracy: 0.9529 - val_loss: 0.1380 - val_accuracy: 0.9453\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
